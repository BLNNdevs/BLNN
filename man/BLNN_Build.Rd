% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BLNN_Build.R
\name{BLNN_Build}
\alias{BLNN_Build}
\title{Build a Feed-Forward Neural Network Structure}
\usage{
BLNN_Build(ncov, nout, hlayer_size = 1, actF = c("linear", "sigmoid",
  "tanh"), costF = c("MSE", "crossEntropy"), outF = c("linear",
  "sigmoid", "softmax"), bias = c(1, 1), hp.Err = FALSE,
  hp.W1 = FALSE, hp.W2 = FALSE, hp.B1 = FALSE, hp.B2 = FALSE,
  decay_term = 1)
}
\arguments{
\item{ncov}{An integer, the number of input units.}

\item{nout}{An integer, the number of output units.}

\item{hlayer_size}{An integer, the number of nodes making up the hidden layer. Default is 1.}

\item{actF}{The choice of activation function. See 'Details'.}

\item{costF}{The choice of cost function. See 'Details'.}

\item{outF}{The choice of output function. See 'Details'.}

\item{bias}{The choice for the bias term of each layer. Defaults is 1 for each layer.}

\item{hp.Err}{Value of the scale hyperparameter for the network errors. Defaults to FALSE for no bayesian.}

\item{hp.W1}{Value of the scale hyperparameter for the first layer weights. Defaults to FALSE for no bayesian.}

\item{hp.W2}{Value of the scale hyperparameter for the second layer weights. Defaults to FALSE for no bayesian.}

\item{hp.B1}{Value of the scale hyperparameter for the first layer bias. Defaults to FALSE for no bayesian.}

\item{hp.B2}{Value of the scale hyperparameter for the second layer bias. Defaults to FALSE for no bayesian.}

\item{decay_term}{Control term for the initial valus of the weights. Standard deciation of initial weights is 1/decay_term. Default is 1.}
}
\value{
The network object, defined as a list containing
\describe{
\item{ncov}{Number of covariates.}

\item{nout}{Number of outputs.}

\item{hidden_size}{Size of the hidden layer.}

\item{actF}{Activation Function.}

\item{costF}{Cost Function.}

\item{bias}{Bias terms.}

\item{scale.error}{Error hyperparameter.}

\item{scale.weights}{A list containing weight and bias hyperparameters.}

\item{weights}{A list containing each layer's weight matrix.}

\item{Amat}{A placeholder to contain weights1 multiplied by inputs.}

\item{postAmat}{A placeholder to contain actF evaluated at Amat.}

\item{Zmat}{A placeholder to contain weights2 multiplied by postAmat.}

\item{trainout}{A placeholder to contain trained output values.}
}
}
\description{
Allows the user to generate a neural network structure for the purposes of training and predicting.
}
\details{
BLNN_Build provides users with different activation, cost, and output finctions
which can be chosen based on the model type. Activation functions are applied at the hidden
layer in order to assist in computation where the output function restricts the range of values to
fit the given problem. We recomend using tanh in the activation function in most cases. MSE can be
used when taking the error of linear outputs but cross entropy is suggested otherwise.
}
